{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irlIEgYU_Bpm"
      },
      "source": [
        "# Superstore PySpark Assignment\n",
        "This notebook answers Questions 9–14 from the Module 1 Assignment using PySpark and the Superstore dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install Java Development Kit (JDK)\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# 2. Download and Extract Spark\n",
        "# Adjust Spark version as needed (e.g., spark-3.5.1-bin-hadoop3)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar -xvf spark-3.4.1-bin-hadoop3.tgz > /dev/null # Extract quietly\n",
        "\n",
        "# 3. Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\" # Ensure this matches the extracted folder name\n",
        "\n",
        "# 4. Install findspark and pyspark (if not already installed or to ensure latest)\n",
        "!pip install findspark pyspark --upgrade -qq\n",
        "\n",
        "# 5. Initialize findspark and Create SparkSession\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"YourColabSparkApp\").getOrCreate()\n",
        "\n",
        "print(\"SparkSession created successfully!\")\n",
        "spark.catalog.listTables() # A quick test\n",
        "\n",
        "# Now you can proceed with your original code:\n",
        "# df = ... (your pandas DataFrame)\n",
        "# spark_df = spark.createDataFrame(df)\n",
        "# spark_df.show()"
      ],
      "metadata": {
        "id": "b5TntJHWXLs-",
        "outputId": "85239ac3-4e4c-4674-aa15-d1d1274063e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.1/434.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.0/203.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "SparkSession created successfully!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wzg_zzLr_Bpn"
      },
      "outputs": [],
      "source": [
        "# Install Spark and Java (only needed on Colab)\n",
        "!apt-get install openjdk-17-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "03LiEOMV_Bpn"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import avg, max as spark_max, count, expr\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import expr\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize Spark\n",
        "spark = SparkSession.builder.appName(\"SuperstoreAssignment\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ncZuHq3_Bpn",
        "outputId": "bbcd13d6-98e8-40a1-c406-1df2211d1adb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-02 20:43:43--  https://raw.githubusercontent.com/SamuelBoye/assignments/refs/heads/main/Superstore-2.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2167101 (2.1M) [text/plain]\n",
            "Saving to: ‘Superstore.csv’\n",
            "\n",
            "Superstore.csv      100%[===================>]   2.07M  7.17MB/s    in 0.3s    \n",
            "\n",
            "2025-06-02 20:43:44 (7.17 MB/s) - ‘Superstore.csv’ saved [2167101/2167101]\n",
            "\n",
            "+--------------------+---------+---------+-----------+------------------+----------------+---------------+--------+-------+--------------------+-----------------+----------+--------+--------------+--------------+-----------+-------------------+------+-------+----------+-----+----------+-----------+-------------+-----------+----------+\n",
            "|            Category|     City|Container|Customer ID|     Customer Name|Customer Segment|     Department|Discount|Item ID|                Item|Number of Records|Order Date|Order ID|Order Priority|Order Quantity|Postal Code|Product Base Margin|Profit| Region|    Row ID|Sales| Ship Date|  Ship Mode|Shipping Cost|      State|Unit Price|\n",
            "+--------------------+---------+---------+-----------+------------------+----------------+---------------+--------+-------+--------------------+-----------------+----------+--------+--------------+--------------+-----------+-------------------+------+-------+----------+-----+----------+-----------+-------------+-----------+----------+\n",
            "|               Paper|  Lombard|Small Box|       3035|       Mark Bailey|     Home Office|Office Supplies|    0.01|  10074|Hammermill CopyPl...|                1| 11/7/2017|   89128|           Low|            10|      60148|               0.36| -76.0|Central|11/18/1951| 53.0| 11/9/2018|Regular Air|            5|   Illinois|         5|\n",
            "|               Paper|  Lombard| Wrap Bag|       3035|       Mark Bailey|     Home Office|Office Supplies|    0.04|  10079|Telephone Message...|                1|11/10/2015|   89128|           Low|            12|      60148|               0.39|  52.0|Central|11/19/1951| 76.0|11/12/2016|Regular Air|            1|   Illinois|         6|\n",
            "| Pens & Art Supplies|Southbury| Wrap Bag|       3385|   Daniel Richmond|       Corporate|Office Supplies|    0.04|  11012|Premium Writing P...|                1|11/22/2016|   88745|           Low|             5|       6488|               0.57| -22.0|   East|  3/1/1961| 16.0|11/23/2017|Express Air|            2|Connecticut|         3|\n",
            "|Binders and Binde...|Coachella|Small Box|       3133|Kristine Singleton|       Corporate|Office Supplies|     0.1|  10646|Fellowes Black Pl...|                1| 1/12/2017|   86789|        Medium|            12|      92236|               0.39|-350.0|   West| 6/27/1961| 65.0| 1/25/2018|Regular Air|            8| California|         6|\n",
            "|        Rubber Bands|Coachella| Wrap Bag|       3133|Kristine Singleton|       Corporate|Office Supplies|    0.03|  10138|Assorted Color Pu...|                1| 1/12/2017|   86789|        Medium|            10|      92236|               0.52|   4.0|   West| 6/28/1961| 19.0| 1/25/2018|Regular Air|            1| California|         2|\n",
            "+--------------------+---------+---------+-----------+------------------+----------------+---------------+--------+-------+--------------------+-----------------+----------+--------+--------------+--------------+-----------+-------------------+------+-------+----------+-----+----------+-----------+-------------+-----------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Replace with your actual raw GitHub CSV URL\n",
        "# csv_url = \"https://raw.githubusercontent.com/SamuelBoye/assignments/refs/heads/main/Superstore-2.csv\"\n",
        "\n",
        "# Download the file\n",
        "!wget -O Superstore.csv $csv_url\n",
        "\n",
        "df = pd.read_csv('https://www.dropbox.com/scl/fi/yjb7qiivbobpg56auoqet/Superstore.csv?rlkey=fjm37jmewusgwn14a7mdhnrjy&dl=1')\n",
        "\n",
        "\n",
        "# # Read into Spark\n",
        "# df = spark.read.csv(\"Superstore.csv\", header=True, inferSchema=True)\n",
        "df = spark.createDataFrame(df)\n",
        "\n",
        "# Cast numeric fields\n",
        "df = df.withColumn(\"Sales\", expr(\"try_cast(Sales as double)\"))\n",
        "df = df.withColumn(\"Profit\", expr(\"try_cast(Profit as double)\"))\n",
        "\n",
        "# Preview data\n",
        "df.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7-LyNga_Bpn"
      },
      "source": [
        "### Q9: Who is the customer that generated the most sales?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8hlXlSn_Bpn",
        "outputId": "76e5f4c0-a6a5-4d18-a8a9-b5f2d32a8242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+----------+\n",
            "|     Customer Name|sum(Sales)|\n",
            "+------------------+----------+\n",
            "|Joshua N. Milligan|  123748.0|\n",
            "+------------------+----------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "top_customer = df.groupBy(\"Customer Name\").sum(\"Sales\").orderBy(\"sum(Sales)\", ascending=False)\n",
        "top_customer.show(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI76-JPI_Bpn"
      },
      "source": [
        "### Q10: Highest average sales per transaction for any city"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Siih4Dfc_Bpo",
        "outputId": "0f9416ef-ef32-428e-f189-683aa5397c91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+\n",
            "|      City|avg_sales|\n",
            "+----------+---------+\n",
            "|Barrington|   7029.0|\n",
            "+----------+---------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "avg_sales = df.groupBy(\"City\").agg(avg(\"Sales\").alias(\"avg_sales\")).orderBy(\"avg_sales\", ascending=False)\n",
        "avg_sales.show(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L75fHD5t_Bpo"
      },
      "source": [
        "### Q11: Highest total profit for any item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsWAeLJo_Bpo",
        "outputId": "928ca256-9220-47df-da96-1ff1cefa2da0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------+\n",
            "|                Item|total_profit|\n",
            "+--------------------+------------+\n",
            "|Global Troy� Exec...|     79506.0|\n",
            "+--------------------+------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Q11: Total profit per item, then find the highest\n",
        "from pyspark.sql.functions import sum as spark_sum\n",
        "\n",
        "df.groupBy(\"Item\") \\\n",
        "  .agg(spark_sum(\"Profit\").alias(\"total_profit\")) \\\n",
        "  .orderBy(\"total_profit\", ascending=False) \\\n",
        "  .show(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sYjiNoG_Bpo"
      },
      "source": [
        "### Q12: Largest number of transactions for any combination of state and customer segment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHvZocQg_Bpo",
        "outputId": "005034ed-4d0d-4bfd-9c0d-3b1fca250302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------------+-------------------+\n",
            "|State|Customer Segment|unique_transactions|\n",
            "+-----+----------------+-------------------+\n",
            "|Texas|       Corporate|                249|\n",
            "+-----+----------------+-------------------+\n",
            "only showing top 1 row\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import countDistinct\n",
        "\n",
        "df.groupBy(\"State\", \"Customer Segment\") \\\n",
        "  .agg(countDistinct(\"Order ID\").alias(\"unique_transactions\")) \\\n",
        "  .orderBy(\"unique_transactions\", ascending=False) \\\n",
        "  .show(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt9mGtvP_Bpo"
      },
      "source": [
        "### Q13: Highest average profit for any city-state combination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igCmAXhp_Bpo",
        "outputId": "195a14ba-4af7-4341-f674-864b7a053741"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------+----------+\n",
            "|        City|  State|avg_profit|\n",
            "+------------+-------+----------+\n",
            "|Douglasville|Georgia|    5379.0|\n",
            "+------------+-------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Filter out known corrupted state values\n",
        "df_clean = df.filter(~col(\"State\").isin(\"Regular Air\", \"Express Air\", \"Delivery Truck\", \"Second Class\", \"First Class\"))\n",
        "\n",
        "# Register new clean temp view\n",
        "df_clean.createOrReplaceTempView(\"superstore\")\n",
        "\n",
        "# Now rerun Q13 query\n",
        "spark.sql(\"\"\"\n",
        "SELECT City, State, ROUND(AVG(Profit), 2) AS avg_profit\n",
        "FROM superstore\n",
        "WHERE City IS NOT NULL AND State IS NOT NULL AND Profit IS NOT NULL\n",
        "GROUP BY City, State\n",
        "ORDER BY avg_profit DESC\n",
        "LIMIT 1\n",
        "\"\"\").show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EoZUJ8WqOK0l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}